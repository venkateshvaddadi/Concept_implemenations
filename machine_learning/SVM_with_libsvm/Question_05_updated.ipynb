{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It returns all list of individual words of the document text.\n",
    "def get_individual_words(document_text):\n",
    "    full_text=document_text\n",
    "    full_text=full_text.strip()\n",
    "    full_text=full_text.replace(\"\\n\", \",\")\n",
    "    full_text=full_text.replace(\"\\t\", \",\")\n",
    "    full_text=full_text.replace(\":\", \",\")\n",
    "    full_text=full_text.replace(\";\", \",\")\n",
    "    full_text=full_text.replace(\" \", \",\")\n",
    "    full_text=full_text.replace(\" \", \",\")\n",
    "    full_text=full_text.replace(\"-\", \",\")\n",
    "    full_text=full_text.replace(\"*\", \",\")\n",
    "    full_text=full_text.replace(\"&\", \",\")\n",
    "    full_text=full_text.replace(\"+\", \",\")\n",
    "    full_text=full_text.replace(\"$\", \",\")\n",
    "    full_text=full_text.replace(\"/\", \",\")\n",
    "    full_text=full_text.replace(\"%\", \",\")\n",
    "\n",
    "    full_text=full_text.lower()\n",
    "    full_words=full_text.split(',')\n",
    "    \n",
    "    return full_words;\n",
    "\n",
    "# it returns the documents and it's labels from corpus \n",
    "def get_documents_labels(file_path):\n",
    "    \n",
    "    file1 = open(file_path, 'r')\n",
    "    Lines = file1.readlines()\n",
    "    \n",
    "    documents=[]\n",
    "    labels=[]\n",
    "    for line in Lines:\n",
    "        temp=line.split('\\t')\n",
    "        #print(temp)\n",
    "        documents.append(temp[0])\n",
    "        if(temp[1]=='0\\n'):\n",
    "            labels.append(-1)\n",
    "        elif(temp[1]=='1\\n'):\n",
    "            labels.append(1)\n",
    "    labels=np.array(labels)\n",
    "    return documents,labels\n",
    "\n",
    "# It returns list of all the words of the documents with repeatation.\n",
    "def get_all_words_of_documents(documents):\n",
    "    full_text=''\n",
    "    \n",
    "    for i in documents:\n",
    "        full_text=full_text+\" \"+i\n",
    "        \n",
    "    full_words=get_individual_words(full_text)\n",
    "    \n",
    "    for i in range(len(full_words)):\n",
    "        full_words[i]=full_words[i].lower()\n",
    "    \n",
    "    \n",
    "    full_words_updated=[]\n",
    "    for individual_word in full_words:\n",
    "        if(len(individual_word)!=0 and individual_word!=' '):\n",
    "            full_words_updated.append(individual_word)\n",
    "    return full_words_updated;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It returns the term frequnecy of the term_t in the document_text\n",
    "def tf(term_t,document_text):\n",
    "    term_t=term_t.lower()\n",
    "    full_words=get_individual_words(document_text)\n",
    "    #print(full_words)\n",
    "    term_t_count=0\n",
    "    if(term_t in full_words):\n",
    "        for i in full_words:\n",
    "           if(i==term_t):\n",
    "               term_t_count=term_t_count+1\n",
    "    #print(term_t_count,len(full_words))\n",
    "    return term_t_count/len(full_words)\n",
    "\n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "# it returns the tf-idf of the term_t of the document_text.\n",
    "def tf_idf_single_term(term_t,document_text):\n",
    "    term_t=term_t.lower()\n",
    "    index=term_dictionary[term_t]\n",
    "    term_freq=tf(term_t,document_text)\n",
    "    document_freq=Document_Frequency[index]\n",
    "    #print(term_freq,document_freq)\n",
    "    if(document_freq>0):\n",
    "        return term_freq*math.log(total_words_N/document_freq)\n",
    "    else:\n",
    "        return term_freq*math.log(total_words_N/1+document_freq)\n",
    "    \n",
    "\n",
    "# it returns the tf-idf of the document_text.\n",
    "# it will be used to represent the document as a feature vector.\n",
    "def tf_idf(document_text):\n",
    "    feature_vector=np.zeros(total_unique_words_V,)\n",
    "    for i in range(len(dictionary)):\n",
    "        feature_vector[i]=tf_idf_single_term(dictionary[i],document_text)\n",
    "    return feature_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def higher_dimension_PCA(X):\n",
    "    # Data Centering\n",
    "    X_mean=np.mean(X,axis=0)\n",
    "    X_centered=X-X_mean\n",
    "    \n",
    "    #checking the mean of the centered Data.\n",
    "    #print(np.mean(X_centered,axis=0))\n",
    "    #print(np.sum(np.mean(X_centered,axis=0)))\n",
    "\n",
    "    # we are calculating the eigen values of the X@XT for calculating the eigen values of the XT@X\n",
    "    \n",
    "    N=X.shape[0]\n",
    "    temp_outer_product=(1/N)*(X_centered@X_centered.T)\n",
    "    temp_eigen_values,temp_eigen_vectors=np.linalg.eigh(temp_outer_product)\n",
    "    \n",
    "    idx = np.argsort(-temp_eigen_values)\n",
    "    temp_eigen_values = temp_eigen_values[idx]\n",
    "    temp_eigen_vectors = temp_eigen_vectors[:,idx]\n",
    "        \n",
    "    \n",
    "    #print(temp_eigen_values.shape)\n",
    "    #print(temp_eigen_vectors.shape)\n",
    "    \n",
    "    \n",
    "    # For calculating the eigenvalues of the XT@X\n",
    "    \n",
    "    #print(temp_eigen_values)\n",
    "    print(N,temp_eigen_values)\n",
    "    norms_of_eigen_vectors=np.sqrt(N*temp_eigen_values)\n",
    "    eigen_vectors=X_centered.T@temp_eigen_vectors\n",
    "    \n",
    "    for i in range(eigen_vectors.shape[1]):\n",
    "        eigen_vectors[:,i]=eigen_vectors[:,i]/norms_of_eigen_vectors[i]\n",
    "    \n",
    "    print('Eigen vectors shape',eigen_vectors.shape)\n",
    "\n",
    "    return eigen_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_centering(X):\n",
    "    X_mean=np.mean(X,axis=0)\n",
    "    X_centered=X-X_mean\n",
    "    return X_centered "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path='movie_reviews/movieReviews1000.txt'\n",
    "\n",
    "# reading a documents and labels from the corpus (text file) \n",
    "documents,labels=get_documents_labels(file_path)\n",
    "\n",
    "# full words of all the documents with repeatation. \n",
    "full_words=get_all_words_of_documents(documents)\n",
    "\n",
    "dictionary = list(set(full_words))\n",
    "dictionary.sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total no of documents: 1000\n",
      "total no of words in the documents(N): 14436\n",
      "total no of unique words(V): 3139\n"
     ]
    }
   ],
   "source": [
    "total_unique_words_V=len(dictionary)\n",
    "total_words_N=len(full_words)\n",
    "total_documents_D=len(documents)\n",
    "\n",
    "print('total no of documents:',total_documents_D)\n",
    "print('total no of words in the documents(N):',total_words_N)\n",
    "print('total no of unique words(V):',total_unique_words_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a term dictionary\n",
    "\n",
    "term_dictionary={}\n",
    "for i in range(len(dictionary)):\n",
    "    term_dictionary[dictionary[i]]=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A very very very slow-moving aimless movie about a distressed drifting young man  \n",
      "term-frequency of word slow: 0.07142857142857142\n"
     ]
    }
   ],
   "source": [
    "#print term-frequency of the slow word in the following sentence.\n",
    "\n",
    "print(documents[0])\n",
    "print(\"term-frequency of word slow:\",tf(\"slow\",documents[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term_Frequecy calculation was completed---------------------\n"
     ]
    }
   ],
   "source": [
    "Term_Frequecy=np.zeros((total_unique_words_V,total_documents_D),dtype='float64')\n",
    "\n",
    "for i in range(len(dictionary)):\n",
    "    for j in range(len(documents)):\n",
    "        Term_Frequecy[i][j]=tf(dictionary[i],documents[j])\n",
    "\n",
    "print(\"Term_Frequecy calculation was completed---------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Frequency calculation was completed---------------------\n"
     ]
    }
   ],
   "source": [
    "#document frequecny calculations\n",
    "# execution is only once for code.............. \n",
    "\n",
    "       \n",
    "Document_Frequency=np.zeros(total_unique_words_V)\n",
    "\n",
    "for i in range(len(dictionary)):\n",
    "    temp=Term_Frequecy[i,:]\n",
    "    document_indices=np.where(temp>0)\n",
    "    document_indices=np.array(document_indices)\n",
    "    Document_Frequency[i]=document_indices.shape[1]\n",
    "\n",
    "print(\"Document Frequency calculation was completed---------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document text:\n",
      " The movie seemed a little slow at first  \n",
      "tf-idf array:\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      "Following elements of document vectors are greater than zero:\n",
      "\n",
      "index-----tf-idf\n",
      "44 ----- 0.4689350322977786\n",
      "207 ----- 0.7008985565263197\n",
      "1063 ----- 0.8974481366205386\n",
      "1632 ----- 0.8430333777133079\n",
      "1804 ----- 0.5544771011575024\n",
      "2372 ----- 1.0598585096368212\n",
      "2474 ----- 0.973215112066828\n",
      "2712 ----- 0.41763884694487186\n"
     ]
    }
   ],
   "source": [
    "document_id=np.random.randint(1,1000)\n",
    "document_vector=tf_idf(documents[document_id])\n",
    "\n",
    "print('Document text:\\n',documents[document_id])\n",
    "print('tf-idf array:\\n',document_vector )\n",
    "\n",
    "print(\"Following elements of document vectors are greater than zero:\\n\")\n",
    "indices=np.where(document_vector>0)\n",
    "print(\"index-----tf-idf\")\n",
    "for i  in indices[0]:\n",
    "    print(i,\"-----\",document_vector[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 3139)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we are representing the document as a vector\n",
    "# it representing the corpus of feature vectors.\n",
    "    \n",
    "document_vector=[]\n",
    "\n",
    "for i in documents:\n",
    "    document_vector.append(tf_idf(i))\n",
    "\n",
    "document_vector=np.array(document_vector)\n",
    "document_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data_shape (1000, 3139)\n",
      "projected_data>shape (1000, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "K=10\n",
    "document_vector_centered=data_centering(document_vector)\n",
    "print('Data_shape',document_vector_centered.shape)\n",
    "pca = PCA(n_components = K)\n",
    "projected_data = pca.fit_transform(document_vector_centered)\n",
    "\n",
    "print('projected_data>shape',projected_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sampling(X,X_labels,total_no_samples):\n",
    "    list_indices=list(range(X.shape[0]))\n",
    "    np.random.shuffle(list_indices)\t\n",
    "    train_indices=list_indices[0:total_no_samples]\n",
    "    test_indices=list_indices[total_no_samples:]\n",
    "    \n",
    "    train_data=(X[train_indices],X_labels[train_indices])\n",
    "    test_data=(X[test_indices],X_labels[test_indices])\n",
    "    return train_data,test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential_sampling(X,X_labels,total_no_samples):\n",
    "    train_data=(X[0:total_no_samples],X_labels[0:total_no_samples])\n",
    "    test_data=(X[total_no_samples:],X_labels[total_no_samples:])\n",
    "    return train_data,test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libsvm.svm import *\n",
    "from libsvm.svmutil import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_cusomized(train_data,labels,C,epsilon,kernel_type):\n",
    "    prob = svm_problem(labels,train_data)\n",
    "\n",
    "    param = svm_parameter()\n",
    "    param.C = C\n",
    "    param.nu=epsilon\n",
    "    \n",
    "    if(kernel_type=='LINEAR'):\n",
    "        param.kernel_type = LINEAR\n",
    "    elif(kernel_type=='POLY'):\n",
    "        param.kernel_type = POLY\n",
    "    elif(kernel_type=='RBF'):\n",
    "        param.kernel_type = RBF\n",
    "    elif(kernel_type=='SIGMOID'):\n",
    "        param.kernel_type = SIGMOID\n",
    "    \n",
    "    m = svm_train(prob, param)\n",
    "\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we are doing lot of experiments with various $kernels$ and with different $C$ values.\n",
    "\n",
    "In the training data we have totally $1000$ samples.\n",
    "\n",
    "We are using first $750$ samples for traing.\n",
    "\n",
    "We are using remaing $250$ samples for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking the first 750 samples for training the data.\n",
    "no_training_samples=750\n",
    "(train_X,train_labels),(test_X,test_labels)=sequential_sampling(projected_data,labels,no_training_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "statastics_linear=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For training:\n",
      "Accuracy = 57.6% (432/750) (classification)\n",
      "For testing:\n",
      "Accuracy = 64% (160/250) (classification)\n",
      "57.599999999999994 64.0\n"
     ]
    }
   ],
   "source": [
    "C=1000000\n",
    "epsilon=0.5\n",
    "kernel_type='LINEAR'\n",
    "\n",
    "m=svm_cusomized(train_X,train_labels,C,epsilon,kernel_type)\n",
    "\n",
    "print(\"For training:\")\n",
    "traing_accuracy=svm_predict(train_labels,train_X,m)[1][0]\n",
    "print(\"For testing:\")\n",
    "testing_accuracy=svm_predict(test_labels,test_X,m)[1][0]\n",
    "\n",
    "print(traing_accuracy,testing_accuracy)\n",
    "\n",
    "statastics_linear.append([kernel_type,C,epsilon,traing_accuracy,testing_accuracy,m.get_nr_sv()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LINEAR', 1, 0.5, 57.199999999999996, 48.0, 666]\n",
      "['LINEAR', 10, 0.5, 58.13333333333334, 52.800000000000004, 647]\n",
      "['LINEAR', 100, 0.5, 60.13333333333334, 54.0, 639]\n",
      "['LINEAR', 1000, 0.5, 60.266666666666666, 54.400000000000006, 634]\n",
      "['LINEAR', 10000, 0.5, 60.266666666666666, 54.400000000000006, 634]\n",
      "['LINEAR', 100000, 0.5, 57.333333333333336, 41.199999999999996, 600]\n",
      "['LINEAR', 1000000, 0.5, 57.599999999999994, 64.0, 609]\n",
      "['LINEAR', 0.01, 0.5, 53.733333333333334, 38.800000000000004, 696]\n",
      "['LINEAR', 0.001, 0.5, 53.733333333333334, 38.800000000000004, 695]\n",
      "['LINEAR', 0.0001, 0.5, 53.733333333333334, 38.800000000000004, 694]\n",
      "['LINEAR', 1e-05, 0.5, 53.733333333333334, 38.800000000000004, 694]\n",
      "['LINEAR', 1e-06, 0.5, 53.733333333333334, 38.800000000000004, 694]\n",
      "['LINEAR', 1e-07, 0.5, 53.733333333333334, 38.800000000000004, 694]\n",
      "['LINEAR', 1e-08, 0.5, 53.733333333333334, 38.800000000000004, 694]\n"
     ]
    }
   ],
   "source": [
    "for i in statastics_linear:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "statastics_poly=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For training:\n",
      "Accuracy = 73.4667% (551/750) (classification)\n",
      "For testing:\n",
      "Accuracy = 55.6% (139/250) (classification)\n",
      "73.46666666666667 55.60000000000001\n"
     ]
    }
   ],
   "source": [
    "C=100000000\n",
    "epsilon=0.5\n",
    "kernel_type='POLY'\n",
    "\n",
    "m=svm_cusomized(train_X,train_labels,C,epsilon,kernel_type)\n",
    "\n",
    "print(\"For training:\")\n",
    "traing_accuracy=svm_predict(train_labels,train_X,m)[1][0]\n",
    "print(\"For testing:\")\n",
    "testing_accuracy=svm_predict(test_labels,test_X,m)[1][0]\n",
    "\n",
    "print(traing_accuracy,testing_accuracy)\n",
    "\n",
    "statastics_poly.append([kernel_type,C,epsilon,traing_accuracy,testing_accuracy,m.get_nr_sv()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['POLY', 1, 0.5, 54.0, 39.2, 694]\n",
      "['POLY', 10, 0.5, 54.666666666666664, 39.2, 691]\n",
      "['POLY', 100, 0.5, 55.60000000000001, 40.0, 686]\n",
      "['POLY', 1000, 0.5, 56.8, 40.400000000000006, 672]\n",
      "['POLY', 10000, 0.5, 58.666666666666664, 41.199999999999996, 666]\n",
      "['POLY', 100000, 0.5, 60.13333333333334, 42.8, 646]\n",
      "['POLY', 1000000, 0.5, 65.86666666666666, 44.4, 636]\n",
      "['POLY', 10000000, 0.5, 72.8, 54.400000000000006, 595]\n",
      "['POLY', 100000000, 0.5, 73.46666666666667, 55.60000000000001, 569]\n",
      "['POLY', 0.1, 0.5, 54.0, 39.2, 696]\n",
      "['POLY', 0.01, 0.5, 53.86666666666666, 38.800000000000004, 697]\n",
      "['POLY', 0.001, 0.5, 53.733333333333334, 38.800000000000004, 694]\n",
      "['POLY', 0.0001, 0.5, 53.733333333333334, 38.800000000000004, 694]\n",
      "['POLY', 1e-05, 0.5, 53.733333333333334, 38.800000000000004, 694]\n",
      "['POLY', 1e-06, 0.5, 53.733333333333334, 38.800000000000004, 694]\n",
      "['POLY', 1e-07, 0.5, 53.733333333333334, 38.800000000000004, 694]\n"
     ]
    }
   ],
   "source": [
    "for i in statastics_poly:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "statastics_rbf=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For training:\n",
      "Accuracy = 69.0667% (518/750) (classification)\n",
      "For testing:\n",
      "Accuracy = 62.4% (156/250) (classification)\n",
      "69.06666666666666 62.4\n"
     ]
    }
   ],
   "source": [
    "C=100000\n",
    "epsilon=0.5\n",
    "kernel_type='RBF'\n",
    "\n",
    "m=svm_cusomized(train_X,train_labels,C,epsilon,kernel_type)\n",
    "\n",
    "print(\"For training:\")\n",
    "traing_accuracy=svm_predict(train_labels,train_X,m)[1][0]\n",
    "print(\"For testing:\")\n",
    "testing_accuracy=svm_predict(test_labels,test_X,m)[1][0]\n",
    "\n",
    "print(traing_accuracy,testing_accuracy)\n",
    "\n",
    "statastics_rbf.append([kernel_type,C,epsilon,traing_accuracy,testing_accuracy,m.get_nr_sv()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RBF', 1, 0.5, 56.53333333333334, 41.6, 691]\n",
      "['RBF', 10, 0.5, 58.266666666666666, 49.2, 662]\n",
      "['RBF', 100, 0.5, 61.73333333333333, 60.4, 648]\n",
      "['RBF', 1000, 0.5, 65.06666666666666, 62.8, 634]\n",
      "['RBF', 10000, 0.5, 64.66666666666666, 62.0, 612]\n",
      "['RBF', 100000, 0.5, 69.06666666666666, 62.4, 597]\n",
      "['RBF', 1000000, 0.5, 74.66666666666667, 60.4, 559]\n",
      "['RBF', 0.1, 0.5, 53.733333333333334, 38.800000000000004, 699]\n",
      "['RBF', 0.01, 0.5, 53.733333333333334, 38.800000000000004, 698]\n",
      "['RBF', 0.001, 0.5, 53.733333333333334, 38.800000000000004, 695]\n",
      "['RBF', 0.0001, 0.5, 53.733333333333334, 38.800000000000004, 694]\n",
      "['RBF', 1e-05, 0.5, 53.733333333333334, 38.800000000000004, 694]\n",
      "['RBF', 1e-06, 0.5, 53.733333333333334, 38.800000000000004, 694]\n",
      "['RBF', 1e-07, 0.5, 53.733333333333334, 38.800000000000004, 694]\n",
      "['RBF', 10000000, 0.5, 66.26666666666667, 57.199999999999996, 376]\n"
     ]
    }
   ],
   "source": [
    "for i in statastics_rbf:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "statastics_sigmoid=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For training:\n",
      "Accuracy = 59.3333% (445/750) (classification)\n",
      "For testing:\n",
      "Accuracy = 55.2% (138/250) (classification)\n",
      "59.333333333333336 55.2\n"
     ]
    }
   ],
   "source": [
    "C=1000000\n",
    "epsilon=0.5\n",
    "kernel_type='SIGMOID'\n",
    "\n",
    "m=svm_cusomized(train_X,train_labels,C,epsilon,kernel_type)\n",
    "\n",
    "print(\"For training:\")\n",
    "traing_accuracy=svm_predict(train_labels,train_X,m)[1][0]\n",
    "print(\"For testing:\")\n",
    "testing_accuracy=svm_predict(test_labels,test_X,m)[1][0]\n",
    "\n",
    "print(traing_accuracy,testing_accuracy)\n",
    "\n",
    "statastics_sigmoid.append([kernel_type,C,epsilon,traing_accuracy,testing_accuracy,m.get_nr_sv()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SIGMOID', 1, 0.5, 56.00000000000001, 40.0, 692]\n",
      "['SIGMOID', 10, 0.5, 57.99999999999999, 48.4, 660]\n",
      "['SIGMOID', 100, 0.5, 59.73333333333334, 54.400000000000006, 527]\n",
      "['SIGMOID', 1000, 0.5, 53.86666666666666, 54.0, 369]\n",
      "['SIGMOID', 10000, 0.5, 57.199999999999996, 50.4, 336]\n",
      "['SIGMOID', 100000, 0.5, 57.333333333333336, 52.800000000000004, 328]\n",
      "['SIGMOID', 1000000, 0.5, 59.333333333333336, 55.2, 312]\n",
      "['SIGMOID', 10000000, 0.5, 59.333333333333336, 55.2, 312]\n",
      "['SIGMOID', 100000000, 0.5, 59.333333333333336, 55.2, 312]\n",
      "['SIGMOID', 0.1, 0.5, 53.733333333333334, 38.800000000000004, 696]\n",
      "['SIGMOID', 0.01, 0.5, 53.733333333333334, 38.800000000000004, 696]\n",
      "['SIGMOID', 0.001, 0.5, 53.733333333333334, 38.800000000000004, 694]\n",
      "['SIGMOID', 0.0001, 0.5, 53.733333333333334, 38.800000000000004, 694]\n",
      "['SIGMOID', 1e-05, 0.5, 53.733333333333334, 38.800000000000004, 694]\n",
      "['SIGMOID', 1e-06, 0.5, 53.733333333333334, 38.800000000000004, 694]\n",
      "['SIGMOID', 1e-07, 0.5, 53.733333333333334, 38.800000000000004, 694]\n"
     ]
    }
   ],
   "source": [
    "for i in statastics_sigmoid:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are the results of above experiment.\n",
    "In the above experiments, the training data is first 750 samples and remaining 250 samples are testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<table>\n",
    "    \n",
    "<tr><td>Kernel</td><td> C</td><td> Epsilon</td><td> Training Accuracy</td><td> Testing Accuracy</td><td> No of support vectors</td></tr>\n",
    "\n",
    "<tr><td>'LINEAR'</td><td> 1</td><td> 0.5</td><td> 57.199999999999996</td><td> 48.0</td><td> 666</td></tr>\n",
    "<tr><td>'LINEAR'</td><td> 10</td><td> 0.5</td><td> 58.13333333333334</td><td> 52.800000000000004</td><td> 647</td></tr>\n",
    "<tr><td>'LINEAR'</td><td> 100</td><td> 0.5</td><td> 60.13333333333334</td><td> 54.0</td><td> 639</td></tr>\n",
    "<tr><td>'LINEAR'</td><td> 1000</td><td> 0.5</td><td> 60.266666666666666</td><td> 54.400000000000006</td><td> 634</td></tr>\n",
    "<tr><td>'LINEAR'</td><td> 10000</td><td> 0.5</td><td> 60.266666666666666</td><td> 54.400000000000006</td><td> 634</td></tr>\n",
    "<tr><td>'LINEAR'</td><td> 100000</td><td> 0.5</td><td> 57.333333333333336</td><td> 41.199999999999996</td><td> 600</td></tr>\n",
    "<tr><td>'LINEAR'</td><td> 1000000</td><td> 0.5</td><td> 57.599999999999994</td><td> 64.0</td><td> 609</td></tr>\n",
    "<tr><td>'LINEAR'</td><td> 0.01</td><td> 0.5</td><td> 53.733333333333334</td><td> 38.800000000000004</td><td> 696</td></tr>\n",
    "<tr><td>'LINEAR'</td><td> 0.001</td><td> 0.5</td><td> 53.733333333333334</td><td> 38.800000000000004</td><td> 695</td></tr>\n",
    "<tr><td>'LINEAR'</td><td> 0.0001</td><td> 0.5</td><td> 53.733333333333334</td><td> 38.800000000000004</td><td> 694</td></tr>\n",
    "<tr><td>'LINEAR'</td><td> 1e-05</td><td> 0.5</td><td> 53.733333333333334</td><td> 38.800000000000004</td><td> 694</td></tr>\n",
    "<tr><td>'LINEAR'</td><td> 1e-06</td><td> 0.5</td><td> 53.733333333333334</td><td> 38.800000000000004</td><td> 694</td></tr>\n",
    "<tr><td>'LINEAR'</td><td> 1e-07</td><td> 0.5</td><td> 53.733333333333334</td><td> 38.800000000000004</td><td> 694</td></tr>\n",
    "<tr><td>'LINEAR'</td><td> 1e-08</td><td> 0.5</td><td> 53.733333333333334</td><td> 38.800000000000004</td><td> 694</td></tr>\n",
    "<tr><td>'POLY'</td><td> 1</td><td> 0.5</td><td> 54.0</td><td> 39.2</td><td> 694</td></tr>\n",
    "<tr><td>'POLY'</td><td> 10</td><td> 0.5</td><td> 54.666666666666664</td><td> 39.2</td><td> 691</td></tr>\n",
    "<tr><td>'POLY'</td><td> 100</td><td> 0.5</td><td> 55.60000000000001</td><td> 40.0</td><td> 686</td></tr>\n",
    "<tr><td>'POLY'</td><td> 1000</td><td> 0.5</td><td> 56.8</td><td> 40.400000000000006</td><td> 672</td></tr>\n",
    "<tr><td>'POLY'</td><td> 10000</td><td> 0.5</td><td> 58.666666666666664</td><td> 41.199999999999996</td><td> 666</td></tr>\n",
    "<tr><td>'POLY'</td><td> 100000</td><td> 0.5</td><td> 60.13333333333334</td><td> 42.8</td><td> 646</td></tr>\n",
    "<tr><td>'POLY'</td><td> 1000000</td><td> 0.5</td><td> 65.86666666666666</td><td> 44.4</td><td> 636</td></tr>\n",
    "<tr><td>'POLY'</td><td> 10000000</td><td> 0.5</td><td> 72.8</td><td> 54.400000000000006</td><td> 595</td></tr>\n",
    "<tr><td>'POLY'</td><td> 100000000</td><td> 0.5</td><td> 73.46666666666667</td><td> 55.60000000000001</td><td> 569</td></tr>\n",
    "<tr><td>'POLY'</td><td> 0.1</td><td> 0.5</td><td> 54.0</td><td> 39.2</td><td> 696</td></tr>\n",
    "<tr><td>'POLY'</td><td> 0.01</td><td> 0.5</td><td> 53.86666666666666</td><td> 38.800000000000004</td><td> 697</td></tr>\n",
    "<tr><td>'POLY'</td><td> 0.001</td><td> 0.5</td><td> 53.733333333333334</td><td> 38.800000000000004</td><td> 694</td></tr>\n",
    "<tr><td>'POLY'</td><td> 0.0001</td><td> 0.5</td><td> 53.733333333333334</td><td> 38.800000000000004</td><td> 694</td></tr>\n",
    "<tr><td>'POLY'</td><td> 1e-05</td><td> 0.5</td><td> 53.733333333333334</td><td> 38.800000000000004</td><td> 694</td></tr>\n",
    "<tr><td>'POLY'</td><td> 1e-06</td><td> 0.5</td><td> 53.733333333333334</td><td> 38.800000000000004</td><td> 694</td></tr>\n",
    "<tr><td>'POLY'</td><td> 1e-07</td><td> 0.5</td><td> 53.733333333333334</td><td> 38.800000000000004</td><td> 694</td></tr>\n",
    "<tr><td>'RBF'</td><td> 1</td><td> 0.5</td><td> 56.53333333333334</td><td> 41.6</td><td> 691</td></tr>\n",
    "<tr><td>'RBF'</td><td> 10</td><td> 0.5</td><td> 58.266666666666666</td><td> 49.2</td><td> 662</td></tr>\n",
    "<tr><td>'RBF'</td><td> 100</td><td> 0.5</td><td> 61.73333333333333</td><td> 60.4</td><td> 648</td></tr>\n",
    "<tr><td>'RBF'</td><td> 1000</td><td> 0.5</td><td> 65.06666666666666</td><td> 62.8</td><td> 634</td></tr>\n",
    "<tr><td>'RBF'</td><td> 10000</td><td> 0.5</td><td> 64.66666666666666</td><td> 62.0</td><td> 612</td></tr>\n",
    "<tr><td>'RBF'</td><td> 100000</td><td> 0.5</td><td> 69.06666666666666</td><td> 62.4</td><td> 597</td></tr>\n",
    "<tr><td>'RBF'</td><td> 1000000</td><td> 0.5</td><td> 74.66666666666667</td><td> 60.4</td><td> 559</td></tr>\n",
    "<tr><td>'RBF'</td><td> 0.1</td><td> 0.5</td><td> 53.733333333333334</td><td> 38.800000000000004</td><td> 699</td></tr>\n",
    "<tr><td>'RBF'</td><td> 0.01</td><td> 0.5</td><td> 53.733333333333334</td><td> 38.800000000000004</td><td> 698</td></tr>\n",
    "<tr><td>'RBF'</td><td> 0.001</td><td> 0.5</td><td> 53.733333333333334</td><td> 38.800000000000004</td><td> 695</td></tr>\n",
    "<tr><td>'RBF'</td><td> 0.0001</td><td> 0.5</td><td> 53.733333333333334</td><td> 38.800000000000004</td><td> 694</td></tr>\n",
    "<tr><td>'RBF'</td><td> 1e-05</td><td> 0.5</td><td> 53.733333333333334</td><td> 38.800000000000004</td><td> 694</td></tr>\n",
    "<tr><td>'RBF'</td><td> 1e-06</td><td> 0.5</td><td> 53.733333333333334</td><td> 38.800000000000004</td><td> 694</td></tr>\n",
    "<tr><td>'RBF'</td><td> 1e-07</td><td> 0.5</td><td> 53.733333333333334</td><td> 38.800000000000004</td><td> 694</td></tr>\n",
    "<tr><td>'RBF'</td><td> 10000000</td><td> 0.5</td><td> 66.26666666666667</td><td> 57.199999999999996</td><td> 376</td></tr>\n",
    "<tr><td>'SIGMOID'</td><td> 1</td><td> 0.5</td><td> 56.00000000000001</td><td> 40.0</td><td> 692</td></tr>\n",
    "<tr><td>'SIGMOID'</td><td> 10</td><td> 0.5</td><td> 57.99999999999999</td><td> 48.4</td><td> 660</td></tr>\n",
    "<tr><td>'SIGMOID'</td><td> 100</td><td> 0.5</td><td> 59.73333333333334</td><td> 54.400000000000006</td><td> 527</td></tr>\n",
    "<tr><td>'SIGMOID'</td><td> 1000</td><td> 0.5</td><td> 53.86666666666666</td><td> 54.0</td><td> 369</td></tr>\n",
    "<tr><td>'SIGMOID'</td><td> 10000</td><td> 0.5</td><td> 57.199999999999996</td><td> 50.4</td><td> 336</td></tr>\n",
    "<tr><td>'SIGMOID'</td><td> 100000</td><td> 0.5</td><td> 57.333333333333336</td><td> 52.800000000000004</td><td> 328</td></tr>\n",
    "<tr><td>'SIGMOID'</td><td> 1000000</td><td> 0.5</td><td> 59.333333333333336</td><td> 55.2</td><td> 312</td></tr>\n",
    "<tr><td>'SIGMOID'</td><td> 10000000</td><td> 0.5</td><td> 59.333333333333336</td><td> 55.2</td><td> 312</td></tr>\n",
    "<tr><td>'SIGMOID'</td><td> 100000000</td><td> 0.5</td><td> 59.333333333333336</td><td> 55.2</td><td> 312</td></tr>\n",
    "<tr><td>'SIGMOID'</td><td> 0.1</td><td> 0.5</td><td> 53.733333333333334</td><td> 38.800000000000004</td><td> 696</td></tr>\n",
    "<tr><td>'SIGMOID'</td><td> 0.01</td><td> 0.5</td><td> 53.733333333333334</td><td> 38.800000000000004</td><td> 696</td></tr>\n",
    "<tr><td>'SIGMOID'</td><td> 0.001</td><td> 0.5</td><td> 53.733333333333334</td><td> 38.800000000000004</td><td> 694</td></tr>\n",
    "<tr><td>'SIGMOID'</td><td> 0.0001</td><td> 0.5</td><td> 53.733333333333334</td><td> 38.800000000000004</td><td> 694</td></tr>\n",
    "<tr><td>'SIGMOID'</td><td> 1e-05</td><td> 0.5</td><td> 53.733333333333334</td><td> 38.800000000000004</td><td> 694</td></tr>\n",
    "<tr><td>'SIGMOID'</td><td> 1e-06</td><td> 0.5</td><td> 53.733333333333334</td><td> 38.800000000000004</td><td> 694</td></tr>\n",
    "<tr><td>'SIGMOID'</td><td> 1e-07</td><td> 0.5</td><td> 53.733333333333334</td><td> 38.800000000000004</td><td> 694</td></tr>\n",
    "â€‹</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following results are the best of the above experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<table>\n",
    "<tr><td>Kernel</td><td>C</td><td>Epsilon</td><td>Traing Accuracy</td><td>Testing Accuracy</td><td>NoSupport Vectors</td></tr>\n",
    "<tr><td>'LINEAR'</td><td> 1000</td><td> 0.5</td><td> 60.266666666666666</td><td> 54.400000000000006</td><td> 634</td></tr>\n",
    "<tr><td>'LINEAR'</td><td> 1000000</td><td> 0.5</td><td> 57.599999999999994</td><td> 64.0</td><td> 609</td></tr>\n",
    "<tr><td>'POLY'</td><td> 100000000</td><td> 0.5</td><td> 73.46666666666667</td><td> 55.60000000000001</td><td> 569</td></tr>\n",
    "<tr><td>'RBF'</td><td> 100000</td><td> 0.5</td><td> 69.06666666666666</td><td> 62.4</td><td> 597</td></tr>\n",
    "<tr><td>'SIGMOID'</td><td> 1000000</td><td> 0.5</td><td> 59.333333333333336</td><td> 55.2</td><td> 312</td></tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have done the experiment with random sampling also.\n",
    "\n",
    "It means, we have selected $750$ samples randomly and we use these 750 samples as training set. \n",
    "\n",
    "Remaining $250$ samples are used for the testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are the results of those experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_experiment(train_X,train_labels,kernel_type):\n",
    "    statastics=[]\n",
    "    C=0.000001\n",
    "    for i in range(14):\n",
    "        epsilon=0.5\n",
    "        m=svm_cusomized(train_X,train_labels,C,epsilon,kernel_type)\n",
    "\n",
    "        print(\"For training:\")\n",
    "        traing_accuracy=svm_predict(train_labels,train_X,m)[1][0]\n",
    "        print(\"For testing:\")\n",
    "        testing_accuracy=svm_predict(test_labels,test_X,m)[1][0]\n",
    "\n",
    "\n",
    "        print([kernel_type,C,epsilon,traing_accuracy,testing_accuracy,m.get_nr_sv()])\n",
    "\n",
    "        statastics.append([kernel_type,C,epsilon,traing_accuracy,testing_accuracy,m.get_nr_sv()])\n",
    "        C=C*10\n",
    "\n",
    "    for i in statastics:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For training:\n",
      "Accuracy = 51.6% (387/750) (classification)\n",
      "For testing:\n",
      "Accuracy = 45.2% (113/250) (classification)\n",
      "['LINEAR', 1e-06, 0.5, 51.6, 45.2, 726]\n",
      "For training:\n",
      "Accuracy = 51.6% (387/750) (classification)\n",
      "For testing:\n",
      "Accuracy = 45.2% (113/250) (classification)\n",
      "['LINEAR', 9.999999999999999e-06, 0.5, 51.6, 45.2, 726]\n",
      "For training:\n",
      "Accuracy = 51.6% (387/750) (classification)\n",
      "For testing:\n",
      "Accuracy = 45.2% (113/250) (classification)\n",
      "['LINEAR', 9.999999999999999e-05, 0.5, 51.6, 45.2, 727]\n",
      "For training:\n",
      "Accuracy = 51.6% (387/750) (classification)\n",
      "For testing:\n",
      "Accuracy = 45.2% (113/250) (classification)\n",
      "['LINEAR', 0.001, 0.5, 51.6, 45.2, 727]\n",
      "For training:\n",
      "Accuracy = 51.8667% (389/750) (classification)\n",
      "For testing:\n",
      "Accuracy = 45.2% (113/250) (classification)\n",
      "['LINEAR', 0.01, 0.5, 51.866666666666674, 45.2, 727]\n",
      "For training:\n",
      "Accuracy = 58.4% (438/750) (classification)\n",
      "For testing:\n",
      "Accuracy = 54% (135/250) (classification)\n",
      "['LINEAR', 0.1, 0.5, 58.4, 54.0, 705]\n",
      "For training:\n",
      "Accuracy = 61.6% (462/750) (classification)\n",
      "For testing:\n",
      "Accuracy = 55.2% (138/250) (classification)\n",
      "['LINEAR', 1.0, 0.5, 61.6, 55.2, 650]\n",
      "For training:\n",
      "Accuracy = 62.1333% (466/750) (classification)\n",
      "For testing:\n",
      "Accuracy = 56% (140/250) (classification)\n",
      "['LINEAR', 10.0, 0.5, 62.133333333333326, 56.00000000000001, 631]\n",
      "For training:\n",
      "Accuracy = 62.6667% (470/750) (classification)\n",
      "For testing:\n",
      "Accuracy = 56.4% (141/250) (classification)\n",
      "['LINEAR', 100.0, 0.5, 62.66666666666667, 56.39999999999999, 617]\n",
      "For training:\n",
      "Accuracy = 64.2667% (482/750) (classification)\n",
      "For testing:\n",
      "Accuracy = 57.2% (143/250) (classification)\n",
      "['LINEAR', 1000.0, 0.5, 64.26666666666667, 57.199999999999996, 611]\n",
      "For training:\n",
      "Accuracy = 64.5333% (484/750) (classification)\n",
      "For testing:\n",
      "Accuracy = 56.8% (142/250) (classification)\n",
      "['LINEAR', 10000.0, 0.5, 64.53333333333333, 56.8, 610]\n",
      "For training:\n",
      "Accuracy = 61.8667% (464/750) (classification)\n",
      "For testing:\n",
      "Accuracy = 56.4% (141/250) (classification)\n",
      "['LINEAR', 100000.0, 0.5, 61.86666666666667, 56.39999999999999, 601]\n",
      "For training:\n",
      "Accuracy = 60.4% (453/750) (classification)\n",
      "For testing:\n",
      "Accuracy = 56% (140/250) (classification)\n",
      "['LINEAR', 1000000.0, 0.5, 60.4, 56.00000000000001, 576]\n",
      "For training:\n",
      "Accuracy = 45.0667% (338/750) (classification)\n",
      "For testing:\n",
      "Accuracy = 51.6% (129/250) (classification)\n",
      "['LINEAR', 10000000.0, 0.5, 45.06666666666666, 51.6, 408]\n",
      "['LINEAR', 1e-06, 0.5, 51.6, 45.2, 726]\n",
      "['LINEAR', 9.999999999999999e-06, 0.5, 51.6, 45.2, 726]\n",
      "['LINEAR', 9.999999999999999e-05, 0.5, 51.6, 45.2, 727]\n",
      "['LINEAR', 0.001, 0.5, 51.6, 45.2, 727]\n",
      "['LINEAR', 0.01, 0.5, 51.866666666666674, 45.2, 727]\n",
      "['LINEAR', 0.1, 0.5, 58.4, 54.0, 705]\n",
      "['LINEAR', 1.0, 0.5, 61.6, 55.2, 650]\n",
      "['LINEAR', 10.0, 0.5, 62.133333333333326, 56.00000000000001, 631]\n",
      "['LINEAR', 100.0, 0.5, 62.66666666666667, 56.39999999999999, 617]\n",
      "['LINEAR', 1000.0, 0.5, 64.26666666666667, 57.199999999999996, 611]\n",
      "['LINEAR', 10000.0, 0.5, 64.53333333333333, 56.8, 610]\n",
      "['LINEAR', 100000.0, 0.5, 61.86666666666667, 56.39999999999999, 601]\n",
      "['LINEAR', 1000000.0, 0.5, 60.4, 56.00000000000001, 576]\n",
      "['LINEAR', 10000000.0, 0.5, 45.06666666666666, 51.6, 408]\n"
     ]
    }
   ],
   "source": [
    "no_training_samples=750\n",
    "(train_X,train_labels),(test_X,test_labels)=random_sampling(projected_data,labels,no_training_samples)\n",
    "\n",
    "kernel_type='LINEAR'\n",
    "svm_experiment(train_X,train_labels,kernel_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is are results of the above experiments (which are trained on random sampled data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "\n",
    "<tr><td>Kernel</td><td>C</td><td>Epsilon</td><td>Traing Accuracy</td><td>Testing Accuracy</td><td>NoSupport Vectors</td></tr>\n",
    "\n",
    "<tr><td>'LINEAR'</td><td> 1</td><td> 0.5</td><td> 58.666666666666664</td><td> 62.8</td><td> 671</td></tr>\n",
    "<tr><td>'LINEAR'</td><td> 10</td><td> 0.5</td><td> 59.73333333333334</td><td> 62.8</td><td> 650</td></tr>\n",
    "<tr><td>'LINEAR'</td><td> 100</td><td> 0.5</td><td> 60.66666666666667</td><td> 64.0</td><td> 640</td></tr>\n",
    "<tr><td>'LINEAR'</td><td> 100</td><td> 0.5</td><td> 60.66666666666667</td><td> 64.0</td><td> 640</td></tr>\n",
    "<tr><td>'LINEAR'</td><td> 1000</td><td> 0.5</td><td> 61.73333333333333</td><td> 62.0</td><td> 637</td></tr>\n",
    "<tr><td>'LINEAR'</td><td> 10000</td><td> 0.5</td><td> 61.46666666666667</td><td> 62.4</td><td> 635</td></tr>\n",
    "<tr><td>'LINEAR'</td><td> 100000</td><td> 0.5</td><td> 56.266666666666666</td><td> 59.599999999999994</td><td> 608</td></tr>\n",
    "<tr><td>'LINEAR'</td><td> 1000000</td><td> 0.5</td><td> 56.13333333333333</td><td> 57.599999999999994</td><td> 598</td></tr>\n",
    "<tr><td>'LINEAR'</td><td> 0.1</td><td> 0.5</td><td> 57.733333333333334</td><td> 61.199999999999996</td><td> 724</td></tr>\n",
    "<tr><td>'LINEAR'</td><td> 0.01</td><td> 0.5</td><td> 50.93333333333333</td><td> 48.4</td><td> 740</td></tr>\n",
    "<tr><td>'LINEAR'</td><td> 0.001</td><td> 0.5</td><td> 50.66666666666667</td><td> 48.0</td><td> 740</td></tr>\n",
    "<tr><td>'LINEAR'</td><td> 0.0001</td><td> 0.5</td><td> 50.66666666666667</td><td> 48.0</td><td> 740</td></tr>\n",
    "<tr><td>'LINEAR'</td><td> 1e-05</td><td> 0.5</td><td> 50.66666666666667</td><td> 48.0</td><td> 740</td></tr>\n",
    "<tr><td>'LINEAR'</td><td> 1e-06</td><td> 0.5</td><td> 50.66666666666667</td><td> 48.0</td><td> 740</td></tr>\n",
    "<tr><td>'LINEAR'</td><td> 1e-07</td><td> 0.5</td><td> 50.66666666666667</td><td> 48.0</td><td> 740</td></tr>\n",
    "<tr><td>'LINEAR'</td><td> 1e-08</td><td> 0.5</td><td> 50.66666666666667</td><td> 48.0</td><td> 740</td></tr>\n",
    "\n",
    "<tr><td>'POLY'</td><td> 1</td><td> 0.5</td><td> 50.93333333333333</td><td> 48.4</td><td> 738</td></tr>\n",
    "<tr><td>'POLY'</td><td> 10</td><td> 0.5</td><td> 51.33333333333333</td><td> 48.8</td><td> 736</td></tr>\n",
    "<tr><td>'POLY'</td><td> 100</td><td> 0.5</td><td> 54.0</td><td> 54.800000000000004</td><td> 728</td></tr>\n",
    "<tr><td>'POLY'</td><td> 1000</td><td> 0.5</td><td> 55.46666666666666</td><td> 55.2</td><td> 700</td></tr>\n",
    "<tr><td>'POLY'</td><td> 10000</td><td> 0.5</td><td> 58.266666666666666</td><td> 56.39999999999999</td><td> 678</td></tr>\n",
    "<tr><td>'POLY'</td><td> 100000</td><td> 0.5</td><td> 60.0</td><td> 56.39999999999999</td><td> 658</td></tr>\n",
    "<tr><td>'POLY'</td><td> 1000000</td><td> 0.5</td><td> 65.33333333333333</td><td> 59.199999999999996</td><td> 625</td></tr>\n",
    "<tr><td>'POLY'</td><td> 10000000</td><td> 0.5</td><td> 67.2</td><td> 56.8</td><td> 598</td></tr>\n",
    "<tr><td>'POLY'</td><td> 100000000</td><td> 0.5</td><td> 72.13333333333334</td><td> 59.199999999999996</td><td> 579</td></tr>\n",
    "<tr><td>'POLY'</td><td> 1000000000</td><td> 0.5</td><td> 74.53333333333333</td><td> 60.0</td><td> 541</td></tr>\n",
    "<tr><td>'POLY'</td><td> 0.1</td><td> 0.5</td><td> 50.93333333333333</td><td> 48.4</td><td> 739</td></tr>\n",
    "<tr><td>'POLY'</td><td> 0.01</td><td> 0.5</td><td> 50.8</td><td> 48.0</td><td> 740</td></tr>\n",
    "<tr><td>'POLY'</td><td> 0.001</td><td> 0.5</td><td> 50.66666666666667</td><td> 48.0</td><td> 740</td></tr>\n",
    "<tr><td>'POLY'</td><td> 0.0001</td><td> 0.5</td><td> 50.66666666666667</td><td> 48.0</td><td> 740</td></tr>\n",
    "<tr><td>'POLY'</td><td> 1e-05</td><td> 0.5</td><td> 50.66666666666667</td><td> 48.0</td><td> 740</td></tr>\n",
    "<tr><td>'POLY'</td><td> 1e-06</td><td> 0.5</td><td> 50.66666666666667</td><td> 48.0</td><td> 740</td></tr>\n",
    "<tr><td>'POLY'</td><td> 1e-07</td><td> 0.5</td><td> 50.66666666666667</td><td> 48.0</td><td> 740</td></tr>\n",
    "<tr><td>'POLY'</td><td> 1e-08</td><td> 0.5</td><td> 50.66666666666667</td><td> 48.0</td><td> 740</td></tr>\n",
    "<tr><td>'POLY'</td><td> 1e-09</td><td> 0.5</td><td> 50.66666666666667</td><td> 48.0</td><td> 740</td></tr>\n",
    "<tr><td>'POLY'</td><td> 1e-10</td><td> 0.5</td><td> 50.66666666666667</td><td> 48.0</td><td> 740</td></tr>\n",
    "\n",
    "<tr><td>'RBF'</td><td> 1</td><td> 0.5</td><td> 59.333333333333336</td><td> 59.199999999999996</td><td> 712</td></tr>\n",
    "<tr><td>'RBF'</td><td> 10</td><td> 0.5</td><td> 59.73333333333334</td><td> 61.199999999999996</td><td> 668</td></tr>\n",
    "<tr><td>'RBF'</td><td> 100</td><td> 0.5</td><td> 61.86666666666667</td><td> 59.599999999999994</td><td> 642</td></tr>\n",
    "<tr><td>'RBF'</td><td> 1000</td><td> 0.5</td><td> 64.13333333333333</td><td> 60.4</td><td> 618</td></tr>\n",
    "<tr><td>'RBF'</td><td> 10000</td><td> 0.5</td><td> 66.4</td><td> 64.0</td><td> 606</td></tr>\n",
    "<tr><td>'RBF'</td><td> 100000</td><td> 0.5</td><td> 70.13333333333334</td><td> 61.199999999999996</td><td> 580</td></tr>\n",
    "<tr><td>'RBF'</td><td> 1000000</td><td> 0.5</td><td> 72.0</td><td> 57.199999999999996</td><td> 543</td></tr>\n",
    "<tr><td>'RBF'</td><td> 0.1</td><td> 0.5</td><td> 50.66666666666667</td><td> 48.0</td><td> 741</td></tr>\n",
    "<tr><td>'RBF'</td><td> 0.01</td><td> 0.5</td><td> 50.66666666666667</td><td> 48.0</td><td> 741</td></tr>\n",
    "<tr><td>'RBF'</td><td> 0.001</td><td> 0.5</td><td> 50.66666666666667</td><td> 48.0</td><td> 741</td></tr>\n",
    "<tr><td>'RBF'</td><td> 0.0001</td><td> 0.5</td><td> 50.66666666666667</td><td> 48.0</td><td> 740</td></tr>\n",
    "<tr><td>'RBF'</td><td> 1e-05</td><td> 0.5</td><td> 50.66666666666667</td><td> 48.0</td><td> 740</td></tr>\n",
    "<tr><td>'RBF'</td><td> 1e-06</td><td> 0.5</td><td> 50.66666666666667</td><td> 48.0</td><td> 740</td></tr>\n",
    "<tr><td>'RBF'</td><td> 1e-07</td><td> 0.5</td><td> 50.66666666666667</td><td> 48.0</td><td> 740</td></tr>\n",
    "<tr><td>'RBF'</td><td> 1e-08</td><td> 0.5</td><td> 50.66666666666667</td><td> 48.0</td><td> 740</td></tr>\n",
    "\n",
    "<tr><td>'SIGMOID'</td><td> 1</td><td> 0.5</td><td> 57.599999999999994</td><td> 61.6</td><td> 722</td></tr>\n",
    "<tr><td>'SIGMOID'</td><td> 10</td><td> 0.5</td><td> 57.733333333333334</td><td> 61.6</td><td> 668</td></tr>\n",
    "<tr><td>'SIGMOID'</td><td> 100</td><td> 0.5</td><td> 59.73333333333334</td><td> 60.4</td><td> 515</td></tr>\n",
    "<tr><td>'SIGMOID'</td><td> 1000</td><td> 0.5</td><td> 59.199999999999996</td><td> 60.8</td><td> 333</td></tr>\n",
    "<tr><td>'SIGMOID'</td><td> 10000</td><td> 0.5</td><td> 59.46666666666667</td><td> 59.199999999999996</td><td> 312</td></tr>\n",
    "<tr><td>'SIGMOID'</td><td> 100000</td><td> 0.5</td><td> 59.333333333333336</td><td> 61.199999999999996</td><td> 312</td></tr>\n",
    "<tr><td>'SIGMOID'</td><td> 1000000</td><td> 0.5</td><td> 59.333333333333336</td><td> 61.199999999999996</td><td> 312</td></tr>\n",
    "<tr><td>'SIGMOID'</td><td> 10000000</td><td> 0.5</td><td> 59.333333333333336</td><td> 61.199999999999996</td><td> 312</td></tr>\n",
    "<tr><td>'SIGMOID'</td><td> 100000000</td><td> 0.5</td><td> 59.333333333333336</td><td> 61.199999999999996</td><td> 312</td></tr>\n",
    "<tr><td>'SIGMOID'</td><td> 1000000000</td><td> 0.5</td><td> 58.8</td><td> 61.199999999999996</td><td> 312</td></tr>\n",
    "<tr><td>'SIGMOID'</td><td> 10000000000</td><td> 0.5</td><td> 58.8</td><td> 61.199999999999996</td><td> 312</td></tr>\n",
    "<tr><td>'SIGMOID'</td><td> 100000000000</td><td> 0.5</td><td> 58.8</td><td> 61.199999999999996</td><td> 312</td></tr>\n",
    "<tr><td>'SIGMOID'</td><td> 1000000000000</td><td> 0.5</td><td> 58.8</td><td> 61.199999999999996</td><td> 312</td></tr>\n",
    "\n",
    "<tr><td>'SIGMOID'</td><td> 0.1</td><td> 0.5</td><td> 50.66666666666667</td><td> 48.0</td><td> 741</td></tr>\n",
    "<tr><td>'SIGMOID'</td><td> 0.01</td><td> 0.5</td><td> 50.66666666666667</td><td> 48.0</td><td> 741</td></tr>\n",
    "<tr><td>'SIGMOID'</td><td> 0.001</td><td> 0.5</td><td> 50.66666666666667</td><td> 48.0</td><td> 740</td></tr>\n",
    "<tr><td>'SIGMOID'</td><td> 0.0001</td><td> 0.5</td><td> 50.66666666666667</td><td> 48.0</td><td> 740</td></tr>\n",
    "<tr><td>'SIGMOID'</td><td> 1e-05</td><td> 0.5</td><td> 50.66666666666667</td><td> 48.0</td><td> 740</td></tr>\n",
    "<tr><td>'SIGMOID'</td><td> 1e-06</td><td> 0.5</td><td> 50.66666666666667</td><td> 48.0</td><td> 740</td></tr>\n",
    "<tr><td>'SIGMOID'</td><td> 1e-07</td><td> 0.5</td><td> 50.66666666666667</td><td> 48.0</td><td> 740</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the best of the above experiments (which are trained on random sampled data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "\n",
    "<tr><td>Kernel</td><td>C</td><td>Epsilon</td><td>Traing Accuracy</td><td>Testing Accuracy</td><td>NoSupport Vectors</td></tr>\n",
    "\n",
    "<tr><td>'LINEAR'</td><td> 1000</td><td> 0.5</td><td> 61.73333333333333</td><td> 62.0</td><td> 637</td></tr>\n",
    "<tr><td>'POLY'</td><td> 1000000000</td><td> 0.5</td><td> 74.53333333333333</td><td> 60.0</td><td> 541</td></tr>\n",
    "<tr><td>'RBF'</td><td> 100000</td><td> 0.5</td><td> 70.13333333333334</td><td> 61.199999999999996</td><td> 580</td></tr>\n",
    "<tr><td>'SIGMOID'</td><td> 10000</td><td> 0.5</td><td> 59.46666666666667</td><td> 59.199999999999996</td><td> 312</td></tr>\n",
    "\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
